# Stock Researcher

This is an AI system which coordinates up to ~90 LLM calls to gather facts in parallel about public companies and turns them into an easy-to-read report. You provide a stock ticker or a list of tickers and it looks up information to answer a set of curated questions and then summarizes whether the company appears to meet each criterion. Questions and points are defined in the `questions/` folder. An example output of the system is provided in example_output.md in the root directory of this repository.

Disclaimer: This project is for informational and educational purposes only. It does not provide financial, investment, or trading advice, and no output generated by this system should be interpreted as a recommendation to buy, sell, or hold any security. The system gathers and summarizes publicly available information using AI but does not verify its accuracy or completeness. Users are solely responsible for any decisions made based on this information. Always conduct independent research and consult a licensed financial advisor before making investment decisions.

---

## How to run a quick test

1) **Requirements**
   - Python 3.10+
   - Instructions shown are for macOS/Linux but it works with Windows as well with some command changes.
   - An OpenAI API key stored in a `.env` file (`OPENAI_API_KEY=...`).

2) **Open Terminal, clone, and enter the folder**
```bash
git clone <your-repo-url> pythonparrot-stock-researcher
cd pythonparrot-stock-researcher
```

3) **Create a virtual environment and install packages**
```bash
python -m venv .venv
source .venv/bin/activate   # On Windows: .venv\Scripts\activate

pip install --upgrade pip
pip install -r requirements.txt
```

4) **Add your API key**
Create a `.env` file in the project folder:
```bash
echo "OPENAI_API_KEY=your_key_here" >> .env
```
The program checks for this key before running. If you do not have an OpenAI API key, refer to this [Link](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://platform.openai.com/api-keys&ved=2ahUKEwjy59COlZGQAxWWMVkFHbtpJYkQFnoECA0QAQ&usg=AOvVaw1YhcGDWJXhiKSfmL59Pnfn$0).

5) **Create an output folder**
```bash
mkdir -p reports
```
Reports are written into `reports/` with a date-stamped filename.

6) **Run an example**
- **BEFORE RUNNING ANY EXAMPLES, KEEP IN MIND THAT YOUR OPENAI ACCOUNT WILL BE BILLED FOR THE TOKENS USED.** Running the first example below should cost less than $1 because it is only for a single point for a single company, but running the program for all questions for a single company may cost several dollars or more. If this is too expensive for practical use, you can greatly reduce costs by switching the reasoning effort in line 59 of researcher.py to "Low", although the amount of information gathered and general model performance will likely be reduced.

- Single company, questions only for Point 1, limit of 5 concurrent calls:
```bash
python main.py AAPL 1 5
```
- Batch mode using the provided list (again, be careful with managing costs):
```bash
python main.py tickers.txt all
```


**What happens when running**
- The program loads a list of questions grouped into points (management, margins, long-term outlook, and others).
- It gathers information from the internet for each question and creates a Markdown report. 
- It adds a “Pass/Fail" summary for each point at the top of the report.
- It saves the result to `reports/` with a name like `AAPL_all_YYYYMMDD.md`.
- The research may take ~30 seconds to 10 minutes depending on the reasoning effort used.
- It is common for the final research report to be 100-200 pages long. This is why a summary is provided; it is useful to read to determine if it is worth diving into the full report.



---

## Repository layout

```
pythonparrot-stock-researcher/
├─ main.py                # Prints usage and runs a session
├─ researcher.py          # Gathers factual notes per question
├─ evaluator.py           # Builds Pass/Fail summaries per point
├─ requirements.txt       # Python dependencies (openai, python-dotenv, pandas)
├─ tickers.txt            # Example list for batch runs
└─ questions/
   ├─ points.csv          # The point titles (high-level topics)
   └─ subquestions.csv    # The actual questions
```

---

## How the pieces fit together

```
Choose tickers + which points to run
                │
                v
             main.py
  (loads questions, prints usage)
                │
                v
          researcher.py
(collects factual notes per question)
                │
                v
           evaluator.py
 (creates Pass/Fail summary per point)
                │
                v
        reports/<file>.md
    (final Markdown report)
```

---

## How to change what gets researched

- Edit **`questions/points.csv`** to rename or reorder the main points. 
- Edit **`questions/subquestions.csv`** to add, remove, or reword the questions.
- Edit **`tickers.txt`** to change the batch list.

---

## Usage quick reference

```
python main.py <stock_ticker|tickers.txt> <point_value|all> [max_concurrent]
```

Examples:
- `python main.py AAPL all` — run all points for Apple.
- `python main.py AAPL 12` — run only point 12 for Apple.
- `python main.py tickers.txt all 10` — run all points for all listed tickers with a maximum of 10 concurrent LLM calls at one time, instead of the default 5 (keep in mind that increasing the max_concurrent argument increases the risk that you hit rate limits, especially at lower OpenAI API tiers).

---
## Final Notes
- The intention of this project is to take the timeless principles of the classic investment book *Common Stocks and Uncommon Profits* by Philip A. Fisher and apply them today using modern tools. The criteria and questions are based on the qualitative characteristics that Fisher deems are important to evaluate when analyzing a company. A more readable version of the criteria and questions used can be found in questions/Researcher Prompts.pdf. The prompts could be made more true to the source by being focused more on commentary from people associated with the business such as employees, industry experts, customers, etc. rather than official news or company disclosures.
- The reason that the LLM's role as an evaluator of the information is limited to a brief summary at the top of the final output is that current LLMs struggle to engage in the intuitive, non-rigid reasoning needed to evaluate the various moving parts and diverse characteristics of a company.
- While LLMs have gotten more reliable since their inception, there is still the risk of hallucinated or irrelevant information--the latter of which was spotted during testing of an earlier iteration of this system--being outputted as a part of the report. One must be careful of this and take measures to verify important, quantitative, or suspicious information presented by the system.
- A company may embody all the traits which would make it a great company, but if those exceptional qualities are already priced in by the market, one would struggle to make excess returns by investing in it. As a result, this system could benefit from being used alongside a valuation-based approach.
