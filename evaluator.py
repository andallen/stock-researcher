from __future__ import annotations
from typing import Dict, List
from openai import OpenAI
from dotenv import load_dotenv
import re

load_dotenv()

CLIENT = OpenAI()

# System prompt to be passed in to the evaluator LLM
# (GPT-5 with high effort thinking) so that it looks at the
# research and determines whether the company passes or fails
# for the criteria present.
EVAL_SYSTEM_PROMPT = (
    "You are an equity research evaluation assistant. "
    "You receive compiled factual research grouped by qualitative points. "
    "For each point, determine Pass or Fail strictly based on the evidence shown. If evidence clearly supports the company passing the criterion, choose Pass. "
    "If evidence is weak, contradictory, absent, or inconclusive, choose Fail. "
    "Return a JSON object with keys as point numbers and values as { 'decision': 'Pass'|'Fail', 'rationale': short justification }. "
    "Keep rationales to <= 3 concise sentences. Make sure rationale is strictly based on the evidence presented."
)


def extract_point_sections(markdown_text: str) -> Dict[int, str]:
    """Take the big markdown report created by the research, find each section
    that starts with a point header, grab the header plus all the text until the next
    point header or the end of file, store each section in a dictionary, and
    return the dictionary. These sections will then be fed into the GPT-5 high-effort
    thinking evaluator so it can judge the company for each point and provide a
    Pass/Fail decision.

    Args:
        markdown_text (str): The complete markdown text generated by the research.

    Returns:
        Dict[int, str]: A dictionary storing key value pairs of each point and the text corresponding with that point.
    """
    # Create a regular expression which captures each point header and all its corresponding text up to the next point header or end of file.
    pattern = re.compile(r"^### Point (\d+) Criterion\n(.*?)(?=^### Point (\d+) Criterion|\Z)", re.MULTILINE | re.DOTALL)
    sections: Dict[int, str] = {} # Dictionary to store point # and corresponding text pairs
    for match in pattern.finditer(markdown_text): # For every block of text that matches the pattern...
        point_num = int(match.group(1)) #... assign the block's point number to point_num...
        block = match.group(0).strip() #... and assign the whole block, including the header, to block.
        sections[point_num] = block # Add the pair to the dictionary.
    return sections


def evaluate_points(markdown_text: str) -> str:
    """Run GPT-5 (high reasoning) to assign Pass/Fail for each point and return a
    markdown-formatted summary list. 

    Args:
        markdown_text: The complete markdown text generated by the research.

    Returns:
        str: Markdown string summarizing Pass/Fail decisions with rationales.
    """
    # Calls extract_point_sections to get a dictionary which holds chunks of the research text corresponding to each point
    sections = extract_point_sections(markdown_text)
    # Return no evaluation if the dictionary is invalid
    if not sections:
        return ""

    # Loop through the sections dictionary, add each point
    # and its text as an element of the compiled array,
    # and then merge it into one big string with a blank
    # line separating each seciton.
    compiled = []
    for key in sections.keys():
        compiled.append(f"POINT {key}\n{sections[key]}\n")
    merged = "\n\n".join(compiled)

    # Prompt to be passed in to the evaluator LLM which includes the merged
    # text of all the research for each point.
    prompt = (
        "Evaluate the following compiled research grouped by point. For each point decide Pass or Fail.\n"
        "Respond ONLY with JSON object mapping the point number to an object: { 'decision': 'Pass'|'Fail', 'rationale': '...'}\n"
        "Do not use any other formatting such as backticks. Only the JSON object should be returned.\n"
        "Your returned JSON object should only have ONE evaluation decision per integer point, there should not be one for each sub-question."
        "If evidence is insufficient, choose Fail.\n\n" + merged
    )

    # Initialize the LLM and pass in the prompt
    resp = CLIENT.responses.create(
        model="gpt-5",
        reasoning={"effort": "high"},
        text={"verbosity": "low"},
        input=[
            {"role": "system", "content": EVAL_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
    )

    response = resp.output_text
    print(response)

    # Parse the JSON returned by the LLM
    import json
    try:
        parsed = json.loads(response)
    except Exception:
        parsed = {}

    # Normalize keys: JSON object keys are always strings; convert numeric-like keys to int
    data: Dict[int, dict] = {}
    if isinstance(parsed, dict):
        for k, v in parsed.items():
            try:
                ik = int(k)
            except (TypeError, ValueError):
                continue
            # Ensure value is a dict with expected fields
            if isinstance(v, dict):
                data[ik] = v
            else:
                data[ik] = {}


    # Build the evaluation summary
    header = "### Evaluation Summary\n\n"
    list_lines: List[str] = []
    for key in sections.keys():
        # Try to get the parsed JSON entry for the point number. If 
        # failed, fall back to an empty dictionary.
        entry = data.get(key) or {}
        decision = entry.get("decision", "Fail") # Get the decision. If failed, fall back to "Fail" as the decision.
        rationale = entry.get("rationale", "No rationale parsed") # Get the rationale. if fail, fall back to "No rationale parsed" as the rationale.
        rationale = str(rationale).replace("\n", " ").strip() # Format the rationale
        # Append the decision and rationale behind the decision for this point to list_lines
        list_lines.append(f"Point {key} Decision: {decision}\nRationale: {rationale}\n")
    list_md = header + "\n".join(list_lines) # Join the evaluation summary header with all the summaries
    return list_md # Return the final string of the evaluation summary header and all the summaries